<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>test | zqy&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Accelerating Multi-Model Inference by Merging DNNs of Different Weights摘要当DNN模型被用来处理不同的任务时，现有的提高GPU利用率的技术并不适用，因为模型由于经过微调不共享权重，本文提出了NetFuse，一种合并多个DNN模型的技术，这些模型共享相同的架构，但具有不同的权重和不同的输入。 简介机器学习任务中存在各种标准化的D">
<meta property="og:type" content="article">
<meta property="og:title" content="test">
<meta property="og:url" content="https://bryndenzh.github.io/2021/03/15/test/index.html">
<meta property="og:site_name" content="zqy&#39;s blog">
<meta property="og:description" content="Accelerating Multi-Model Inference by Merging DNNs of Different Weights摘要当DNN模型被用来处理不同的任务时，现有的提高GPU利用率的技术并不适用，因为模型由于经过微调不共享权重，本文提出了NetFuse，一种合并多个DNN模型的技术，这些模型共享相同的架构，但具有不同的权重和不同的输入。 简介机器学习任务中存在各种标准化的D">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-03-15T08:32:20.000Z">
<meta property="article:modified_time" content="2021-03-15T08:33:16.066Z">
<meta property="article:author" content="zqy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="zqy's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zqy&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://bryndenzh.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/15/test/" class="article-date">
  <time class="dt-published" datetime="2021-03-15T08:32:20.000Z" itemprop="datePublished">2021-03-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      test
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Accelerating-Multi-Model-Inference-by-Merging-DNNs-of-Different-Weights"><a href="#Accelerating-Multi-Model-Inference-by-Merging-DNNs-of-Different-Weights" class="headerlink" title="Accelerating Multi-Model Inference by Merging DNNs of Different Weights"></a>Accelerating Multi-Model Inference by Merging DNNs of Different Weights</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>当DNN模型被用来处理不同的任务时，现有的提高GPU利用率的技术并不适用，因为模型由于经过微调不共享权重，本文提出了NetFuse，一种合并多个DNN模型的技术，这些模型共享相同的架构，但具有不同的权重和不同的输入。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>机器学习任务中存在各种标准化的DNN模型，例如NLP中常用BERT和XLNet等注意力模型，图像分类中常用的ResNet, Inception等。通过迁移学习，在一个任务上学习到的知识可以应用到另一个任务上，（比如）。对特定的任务，一般从网上下载预训练好的参数，经过微调即可使其适应专门的任务。 这导致了不同任务的模型具有相似的结构，但参数不同。</p>
<p>一个服务器集群上提供多个这样的DNN推理任务时，GPU的利用率很低。这主要是由于DNN推理不涉及反向传播，浮点运算数量少。批处理是一种提高GPU利用率的方法，但它一般仅限单个模型。本文提出了NetFuse技术，用于合并具有不同参数和不同输入的多个DNN模型。</p>
<h2 id="合并操作"><a href="#合并操作" class="headerlink" title="合并操作"></a>合并操作</h2><h3 id="矩阵相乘"><a href="#矩阵相乘" class="headerlink" title="矩阵相乘"></a>矩阵相乘</h3><p>矩阵相乘（如全连接层），可以合并为 batch matrix multiplication。单个任务对应一个输入向量与一个权重矩阵相乘，合并不同任务时，将它们的输入向量和权重矩阵均在batch维度上拼接后再相乘。多个独立的相乘任务合并为一个 batch matrix multiplication</p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>多个卷积运算可以合并为分组卷积运算（grouped convolution）。与原始的卷积运算类似，但有一个限制，即每个输出通道只能由某一组输入通道计算，而不是所有的输入通道。每组对应一个输入-权重对（input-weight pair）。通过拼接 channel 维度的输入来合并两个卷积，然后进行分组卷积运算。</p>
<h3 id="层规范化（Layer-normalization）"><a href="#层规范化（Layer-normalization）" class="headerlink" title="层规范化（Layer normalization）"></a>层规范化（Layer normalization）</h3><p>与分组卷积类似，多个实例的 layer normalization 可以合并为 group normalization，将不同实例的输入在通道维度拼接，合成一个大的输入张量，创建一个group normalization 实例，每个组对应一个 input -weight pair。</p>
<h3 id="不可训练的操作"><a href="#不可训练的操作" class="headerlink" title="不可训练的操作"></a>不可训练的操作</h3><p>如激活函数，最大池化等，由于不存在权值，可以直接合并。</p>
<h1 id="PipeSwitch"><a href="#PipeSwitch" class="headerlink" title="PipeSwitch"></a>PipeSwitch</h1><h2 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h2><p>深度学习的任务包括训练和推理任务。训练任务通常吞吐量较高，推理任务要求低延迟。为了满足他们不同的要求，目前的主要做法是为训练和推理分别提供专用的GPU集群。</p>
<p>这样会导致低效：</p>
<ul>
<li>对推理任务来说，由于要满足其严格的服务水平目标（Service-Level Objectives），推理集群的供应一般过度，它们不总是以高利用率运行。</li>
<li>训练集群不能抢占训练任务用于推理任务。</li>
</ul>
<p>调查显示，训练集群和推理集群通常具有相当的计算能力，推理工作负载与用户数量相关，并且在每天显示明确的高峰和低估。在不繁忙的时间应用推理集群进行训练能大大提高利用率。理想情况下，多个深度学习应用程序应该能被部署到一个GPU服务器上，通过像操作系统一样通过任务调度和上下文切换，提高GPU的利用率。</p>
<p>但是，GPU在切换任务时有很高的开销，不能满足推理任务的时间需求，因为推理任务通常在几十到几百毫秒之间，而GPU切换到没有预加载的DNN模型，需要数秒。现有的解决方案大多是不同任务共享GPU内存，要求DNN模型预先加载进GPU内存中。但是GPU内存相比主存更有限，难以加载多个模型。有时，一个训练任务就可能用尽所有显存。而且，这样没有提供应用程序间的显存隔离。</p>
<p>本文提出 PipeSwitch，支持GPU服务器上的多个应用程序高效的多路复用，利用深度学习应用程序的特性实现毫秒级的任务切换开销，满足SLO需求，提高GPU利用率。</p>
<h2 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h2><p>controller、memory daemon、一个active worker，多个 standby workers。</p>
<p>控制器将从客户端接收到的一组任务排成队列，根据设定的调度策略进行调度。如果控制器决定调度，它通知active worker 停止，通知 standby worker 初始化。active worker 停止后，控制器通知 memory daemon 和 standby worker 以流水线方式将任务加载到GPU运行。memory daemon 将内存分配给 standby worker 并将模型传输到GPU中，standby worker 成为新的 active worker。</p>
<h3 id="任务切换开销分析"><a href="#任务切换开销分析" class="headerlink" title="任务切换开销分析"></a>任务切换开销分析</h3><p>将总时间划分为四个阶段：旧任务清理、新任务初始化、GPU内存分配、通过PCIe从CPU传输数据到GPU。</p>
<h3 id="流水线传输模型"><a href="#流水线传输模型" class="headerlink" title="流水线传输模型"></a>流水线传输模型</h3><p>DNN 模型具有分层结构，计算模式也是逐层向后。一个任务不需要等到整个模型加载到GPU上就可以开始计算，可以逐层进行。</p>
<p>最简单的方式是逐层流水线，即传输一层计算一层。但由于传输数据需要调用PCIe，多次调用会增大开销，同时也会增大传输和计算之间同步导致的开销。本文将网络多个层合并为一个组，流水线以组为单元执行，降低开销。</p>
<p>F(B, i) 表示：选定网络的前 i 层作为第一组，同时 i + 1 到第 n 层采用最优分组策略，消耗的总时间，可以通过递归求解。</p>
<p>本文进一步提出两种剪枝策略。首先，不需要检查所有 n 中情况，如果第一组包含太多层，第一组的计算将会有很大的延迟。设 T(i, j) 是 i 到 j 层的传输时间，E(i, j) 是计算时间。如图，F 存在一个下界，因此考虑 i 的取值时，如果它对应的下界已经比当前最优情况大，这种情况就可以裁剪。</p>
<p>第二，除了第一组，根据计算进度，可以将多个层打包在一组中，同时不影响流水线的效率。如图，当第一组传输结束时，应当选择至少 j* 层，保证第二组的传输时间不小于第一组的计算时间，以减小传输次数。可以裁剪掉 j &lt; j* 的情况。</p>
<p>具体算法递归，复杂度为O(2^n)，但通过如上裁剪可显著降低复杂度。</p>
<h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p>通常，应用程序通过本地的 cudaMallocManaged 函数分配GPU内存，并将模型的传输委托给 CUDA 管理。深度学习应用程序模型很大，且产生大量中间结果，需要大量GPU内存，CUDA内存管理是为通用应用程序设计的，会带来不必要的开销。</p>
<p>深度学习任务主要存储DNN模型和中间结果，模型占用的内存大小是固定的，而中间结果以简单的规则变化。对于推理任务，只有前向传播，中间结果是每一层的输出，计算下一层后就不要再用，可以释放。反向传播时中间结果以先入后出的方式使用，可以使用堆栈处理。</p>
<p>基于这两个特点，设计内存守护进程管理GPU内存。它在系统启动时使用 cudaMalloc 获取GPU内存，之后在运行时给任务动态分配，消除了任务自行获取内存的开销。不同的任务可能需要同样的模型，但模型只在统一的内存守护进程中存储，减少了内存占用。</p>
<p>降低 IPC 开销。模型传输到GPU后，内存守护进程需要通知 worker，并将内存处理程序到处给 worker。可以通过 GPU 的 IPC 实现，但是多次调用开销较高。由于DNN的内存分配是确定的，也就是说，给定相同的GPU内存区域和相同的模型，只要内存守护进程和工作程序使用相同的顺序为模型参数分配内存，参数的内存指针将是相同的。因此，内存守护进程可以只使用GPU IPC一次初始化worker，然后使用开销较小的CPU IPC通知worker哪个流水线组已经传输。</p>
<h3 id="Active-Standby-Worker-Switching"><a href="#Active-Standby-Worker-Switching" class="headerlink" title="Active-Standby Worker Switching"></a>Active-Standby Worker Switching</h3><p>为了保证任务间快速切换和隔离，主要有两种方案。一是不同任务使用单独的进程，发生抢占时，就进程清理新进程初始化造成较大的开销。另一种是让两个任务使用同一个 CUDA 上下文，重用 GPU 环境，但这仍然有旧进程清理的开销，并且没有提供进程级别的隔离。</p>
<p>本文采用 active-standby worker 机制，每个 worker 是一个单独的进程，被创建时初始化自己的CUDA上下文，这样发生切换时消除了建立环境的开销。任务停止时，需要释放内存。但由于GPU内存是由PipeSwitch 管理的，旧任务只是删除指向张量数据的指针，新任务的传输可以与这个过程并行，消除了任务清理的开销。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bryndenzh.github.io/2021/03/15/test/" data-id="ckmae4r550001zwtm3t4xaj01" data-title="test" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/03/15/%E6%B5%8B%E8%AF%95%E5%9B%BE%E7%89%87/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          测试图片
        
      </div>
    </a>
  
  
    <a href="/2021/03/14/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/03/15/%E6%B5%8B%E8%AF%95%E5%9B%BE%E7%89%87/">测试图片</a>
          </li>
        
          <li>
            <a href="/2021/03/15/test/">test</a>
          </li>
        
          <li>
            <a href="/2021/03/14/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 zqy<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>